{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# ArchBuddy: Complete Project Notebook\n",
                "\n",
                "This notebook contains the entire source code of the ArchBuddy project, organized by component. \n",
                "Run the cells in order to initialize the system."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Universal Imports\n",
                "import logging\n",
                "import sys\n",
                "import os\n",
                "import time\n",
                "import hashlib\n",
                "from typing import List, Dict, Any, Optional, TypedDict, Annotated, Literal, cast, Generator\n",
                "from operator import add\n",
                "from pydantic import BaseModel, Field\n",
                "\n",
                "# LangChain Imports\n",
                "from langchain_core.messages import SystemMessage, HumanMessage, AIMessage, ToolMessage\n",
                "from langchain_core.tools import Tool\n",
                "from langchain_openai import ChatOpenAI\n",
                "from langchain_community.utilities import GoogleSerperAPIWrapper\n",
                "from langchain_chroma import Chroma\n",
                "from langchain_ollama.embeddings import OllamaEmbeddings\n",
                "from langgraph.graph import StateGraph, START, END\n",
                "from langgraph.checkpoint.memory import MemorySaver\n",
                "\n",
                "# Setup Logging\n",
                "logging.basicConfig(\n",
                "    level=logging.INFO,\n",
                "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
                "    force=True\n",
                ")\n",
                "logger = logging.getLogger(__name__)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Core Types (`core/types.py`)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def merge_dicts(left: Dict[str, Any], right: Dict[str, Any]) -> Dict[str, Any]:\n",
                "    \"\"\"\n",
                "    Deep merge two dictionaries.\n",
                "    Used when multiple nodes update nested structures.\n",
                "    \"\"\"\n",
                "    result = left.copy()\n",
                "    for key, value in right.items():\n",
                "        if key in result and isinstance(result[key], dict) and isinstance(value, dict):\n",
                "            result[key] = merge_dicts(result[key], value)\n",
                "        else:\n",
                "            result[key] = value\n",
                "    return result\n",
                "\n",
                "def last_value(left: Any, right: Any) -> Any:\n",
                "    \"\"\"Simple 'last one wins' reducer.\"\"\"\n",
                "    return right\n",
                "\n",
                "class ArchitectureState(TypedDict):\n",
                "    \"\"\"\n",
                "    The complete state for architecture generation.\n",
                "    \"\"\"\n",
                "    # ========== METADATA ==========\n",
                "    messages: Annotated[List, add]\n",
                "    user_problem: Annotated[str, last_value]\n",
                "    iteration_count: Annotated[int, last_value]\n",
                "    min_iterations: Annotated[int, last_value]\n",
                "    max_iterations: Annotated[int, last_value]\n",
                "    \n",
                "    # ========== GENERATION PHASE ==========\n",
                "    architecture_domain_tasks: Annotated[Dict[str, Dict[str, Any]], merge_dicts]\n",
                "    architecture_components: Annotated[Dict[str, Dict[str, Any]], merge_dicts]\n",
                "    proposed_architecture: Annotated[Dict[str, Any], merge_dicts]\n",
                "    \n",
                "    # ========== VALIDATION PHASE ==========\n",
                "    validation_feedback: Annotated[List[Dict[str, Any]], add]\n",
                "    validation_summary: Annotated[Optional[str], last_value]\n",
                "    \n",
                "    # ========== DECISION ==========\n",
                "    has_validation_errors: Annotated[bool, lambda a, b: b]\n",
                "    \n",
                "    # ========== FINAL OUTPUT ==========\n",
                "    final_architecture: Annotated[Optional[Dict[str, Any]], last_value]\n",
                "    architecture_summary: Annotated[Optional[str], last_value]\n",
                "\n",
                "def create_initial_state(\n",
                "    user_problem: str,\n",
                "    min_iterations: int = 2,\n",
                "    max_iterations: int = 3\n",
                ") -> ArchitectureState:\n",
                "    \"\"\"Create the initial state for a new architecture generation run.\"\"\"\n",
                "    return {\n",
                "        \"messages\": [HumanMessage(content=user_problem)],\n",
                "        \"user_problem\": user_problem,\n",
                "        \"iteration_count\": 0,\n",
                "        \"min_iterations\": min_iterations,\n",
                "        \"max_iterations\": max_iterations,\n",
                "        \"architecture_domain_tasks\": {},\n",
                "        \"architecture_components\": {},\n",
                "        \"proposed_architecture\": {},\n",
                "        \"validation_feedback\": [],\n",
                "        \"validation_summary\": None,\n",
                "        \"has_validation_errors\": False,\n",
                "        \"final_architecture\": None,\n",
                "        \"architecture_summary\": None\n",
                "    }"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Schemas (`core/schemas.py`)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class DomainTask(BaseModel):\n",
                "    \"\"\"A single task for one domain architect.\"\"\"\n",
                "    domain: str = Field(description=\"Domain name: compute, network, storage, or database\")\n",
                "    task_description: str = Field(description=\"What should this domain architect do?\")\n",
                "    requirements: List[str] = Field(description=\"Key requirements for this domain\")\n",
                "    deliverables: List[str] = Field(description=\"What should the architect produce?\")\n",
                "\n",
                "class TaskDecomposition(BaseModel):\n",
                "    \"\"\"Complete task decomposition from supervisor.\"\"\"\n",
                "    user_problem: str\n",
                "    decomposed_tasks: List[DomainTask]\n",
                "    overall_architecture_goals: List[str]\n",
                "    constraints: List[str]\n",
                "\n",
                "class ValidationTask(BaseModel):\n",
                "    \"\"\"A validation task for one domain validator.\"\"\"\n",
                "    domain: str\n",
                "    components_to_validate: List[str]\n",
                "    validation_focus: str\n",
                "\n",
                "class ValidationDecomposition(BaseModel):\n",
                "    \"\"\"Validation tasks from validator supervisor.\"\"\"\n",
                "    validation_tasks: List[ValidationTask]"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Tools (`core/tools.py`)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class ToolManager:\n",
                "    \"\"\"Centralized tool management.\"\"\"\n",
                "    \n",
                "    def __init__(self):\n",
                "        self.web_search_tool = self._init_web_search()\n",
                "        self.rag_tool = self._init_rag()\n",
                "    \n",
                "    def _init_web_search(self) -> Tool:\n",
                "        \"\"\"Initialize Google Serper for internet search.\"\"\"\n",
                "        try:\n",
                "            serper = GoogleSerperAPIWrapper()\n",
                "            return Tool(\n",
                "                name=\"web_search\",\n",
                "                func=serper.run,\n",
                "                description=\"Search the internet for current information about AWS services and best practices\"\n",
                "            )\n",
                "        except Exception as e:\n",
                "             # Fallback if Serper not configured\n",
                "             return Tool(\n",
                "                name=\"web_search\",\n",
                "                func=lambda x: \"Web search unavailable (check API key)\",\n",
                "                description=\"Search the internet\"\n",
                "            )\n",
                "    \n",
                "    def _init_rag(self) -> Tool:\n",
                "        \"\"\"Initialize RAG search for vector database.\"\"\"\n",
                "        try:\n",
                "            embeddings = OllamaEmbeddings(model=\"nomic-embed-text\")\n",
                "            # Note: Path adjusted for notebook execution context\n",
                "            db_path = \"../chroma_db_AWSDocs\" if os.path.exists(\"../chroma_db_AWSDocs\") else \"./chroma_db_AWSDocs\"\n",
                "            \n",
                "            vector_store = Chroma(\n",
                "                collection_name=\"AWSDocs\",\n",
                "                persist_directory=db_path,\n",
                "                embedding_function=embeddings,\n",
                "            )\n",
                "            \n",
                "            def rag_search(query: str, k: int = 5) -> str:\n",
                "                try:\n",
                "                    docs = vector_store.similarity_search(query, k=k)\n",
                "                    if not docs:\n",
                "                         return \"No relevant documentation found.\"\n",
                "                    results = []\n",
                "                    for i, doc in enumerate(docs, 1):\n",
                "                        content = doc.page_content.strip()[:1000]\n",
                "                        results.append(f\"[Document {i}]:\\n{content}\\n\")\n",
                "                    return \"\\n---\\n\".join(results)\n",
                "                except Exception as e:\n",
                "                    logging.error(f\"RAG error: {str(e)}\")\n",
                "                    return f\"Error: {str(e)}\"\n",
                "            \n",
                "            return Tool(\n",
                "                name=\"RAG_search\",\n",
                "                func=rag_search,\n",
                "                description=\"Search AWS documentation for accurate architectural guidance and best practices\"\n",
                "            )\n",
                "        except Exception as e:\n",
                "            return Tool(\n",
                "                name=\"RAG_search\",\n",
                "                func=lambda x: \"RAG unavailable\",\n",
                "                description=\"Search AWS documentation\"\n",
                "            )\n",
                "    \n",
                "    def get_all_tools(self) -> Dict[str, Tool]:\n",
                "        return {\n",
                "            self.web_search_tool.name: self.web_search_tool,\n",
                "            self.rag_tool.name: self.rag_tool,\n",
                "        }\n",
                "\n",
                "class LLMManager:\n",
                "    \"\"\"Manage LLM instances.\"\"\"\n",
                "    \n",
                "    def __init__(self):\n",
                "        self.mini_llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
                "        self.reasoning_llm = ChatOpenAI(model=\"gpt-4o\")\n",
                "    \n",
                "    def get_mini_llm(self):\n",
                "        return self.mini_llm\n",
                "    \n",
                "    def get_reasoning_llm(self):\n",
                "        return self.reasoning_llm\n",
                "    \n",
                "    def get_mini_with_tools(self, tools: list):\n",
                "        return self.mini_llm.bind_tools(tools)\n",
                "    \n",
                "    def get_reasoning_structured(self, schema):\n",
                "        return self.reasoning_llm.with_structured_output(schema)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Execution Logic (`core/execution.py`)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def execute_tool_calls(\n",
                "    messages: List,\n",
                "    llm_with_tools,\n",
                "    tools: Dict[str, Tool],\n",
                "    max_iterations: int = 3,\n",
                "    timeout: Optional[float] = 60.0,\n",
                "    retry_attempts: int = 2\n",
                ") -> AIMessage:\n",
                "    \"\"\"Execute LLM with tool calling loop.\"\"\"\n",
                "    \n",
                "    tool_iterations = 0\n",
                "    final_response = None\n",
                "    start_time = time.time()\n",
                "    \n",
                "    while tool_iterations < max_iterations:\n",
                "        if timeout and (time.time() - start_time) > timeout:\n",
                "            logger.warning(f\"Timeout after {timeout}s\")\n",
                "            break\n",
                "        \n",
                "        response = None\n",
                "        for attempt in range(retry_attempts + 1):\n",
                "            try:\n",
                "                response = llm_with_tools.invoke(messages)\n",
                "                break\n",
                "            except Exception as e:\n",
                "                if attempt < retry_attempts:\n",
                "                    time.sleep(2 ** attempt)\n",
                "                else:\n",
                "                    return AIMessage(content=f\"Error: {str(e)}\")\n",
                "        \n",
                "        if not response or not hasattr(response, \"content\"):\n",
                "            break\n",
                "        \n",
                "        if hasattr(response, \"tool_calls\") and response.tool_calls:\n",
                "            messages.append(response)\n",
                "            for tool_call in response.tool_calls:\n",
                "                tool_name = tool_call[\"name\"]\n",
                "                if tool_name in tools:\n",
                "                    try:\n",
                "                        tool_args = tool_call.get(\"args\", {})\n",
                "                        if not isinstance(tool_args, dict):\n",
                "                            tool_args = {\"query\": str(tool_args)}\n",
                "                        tool_result = tools[tool_name].invoke(tool_args)\n",
                "                        messages.append(ToolMessage(\n",
                "                            content=str(tool_result),\n",
                "                            tool_call_id=tool_call[\"id\"]\n",
                "                        ))\n",
                "                    except Exception as e:\n",
                "                        messages.append(ToolMessage(\n",
                "                            content=f\"Error: {str(e)}\",\n",
                "                            tool_call_id=tool_call[\"id\"]\n",
                "                        ))\n",
                "                else:\n",
                "                    messages.append(ToolMessage(\n",
                "                        content=f\"Unknown tool: {tool_name}\",\n",
                "                        tool_call_id=tool_call[\"id\"]\n",
                "                    ))\n",
                "            tool_iterations += 1\n",
                "        else:\n",
                "            final_response = response\n",
                "            break\n",
                "            \n",
                "    if final_response is None:\n",
                "        for msg in reversed(messages):\n",
                "            if isinstance(msg, AIMessage):\n",
                "                final_response = msg\n",
                "                break\n",
                "        if final_response is None:\n",
                "            final_response = AIMessage(content=\"Tool execution incomplete\")\n",
                "            \n",
                "    return final_response\n",
                "\n",
                "def detect_errors_llm(validation_result: str) -> bool:\n",
                "    \"\"\"Use LLM to detect if validation found errors.\"\"\"\n",
                "    try:\n",
                "        # Start a local manager for this util\n",
                "        llm_manager = LLMManager()\n",
                "        mini_llm = llm_manager.get_mini_llm()\n",
                "        \n",
                "        max_length = 1000\n",
                "        truncated = validation_result[:1000] if len(validation_result) > 1000 else validation_result\n",
                "        \n",
                "        error_detection_prompt = f\"\"\"\n",
                "Analyze this validation result and determine if it indicates any errors or issues.\n",
                "Validation Result:\n",
                "{truncated}\n",
                "Respond with ONLY the word \"YES\" if there are errors, or ONLY the word \"NO\" if everything is valid.\n",
                "\"\"\"\n",
                "        response = mini_llm.invoke([SystemMessage(content=error_detection_prompt)])\n",
                "        result_text = getattr(response, \"content\", \"\").strip().upper()\n",
                "        \n",
                "        if result_text.startswith(\"YES\"):\n",
                "            return True\n",
                "        elif result_text.startswith(\"NO\"):\n",
                "            return False\n",
                "        else:\n",
                "            strong_indicators = [\"error\", \"incorrect\", \"invalid\", \"misconfiguration\"]\n",
                "            return any(kw in validation_result.lower() for kw in strong_indicators)\n",
                "    except Exception as e:\n",
                "        return True"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Architects (`nodes/architects.py`)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def format_component_recommendations(domain_name: str, task_info: Dict[str, Any], generated_text: Optional[str]) -> str:\n",
                "    if generated_text and generated_text.strip():\n",
                "        return generated_text.strip()\n",
                "    return f\"*(Generated content unavailable for {domain_name})*\"\n",
                "\n",
                "def generic_domain_architect(state: ArchitectureState, domain: str, domain_services: str, llm_manager, tool_manager, timeout: float = 120.0) -> ArchitectureState:\n",
                "    node_name = f\"{domain}_architect\"\n",
                "    logger.info(f\"--- {domain.capitalize()} Architect ---\")\n",
                "    start_time = time.time()\n",
                "    try:\n",
                "        domain_task = state[\"architecture_domain_tasks\"].get(domain, {})\n",
                "        if not domain_task:\n",
                "            return {\"architecture_components\": {domain: {\"recommendations\": \"No task\", \"error\": \"No task\"}}}\n",
                "        \n",
                "        validation_feedback = state.get(\"validation_feedback\", [])\n",
                "        domain_feedback = [fb for fb in validation_feedback if isinstance(fb, dict) and fb.get(\"domain\", \"\").lower() == domain.lower()]\n",
                "        feedback_context = \"\"\n",
                "        if domain_feedback:\n",
                "            feedback_context = \"\\n\\n**Issues Found in Previous Validation:**\\n\" + \"\\n\".join([f\"- {fb.get('result', '')[:200]}\" for fb in domain_feedback])\n",
                "        \n",
                "        system_prompt = f\"\"\"\n",
                "You are an AWS {domain.capitalize()} Domain Architect.\n",
                "Your expertise: {domain_services}\n",
                "**Problem**: {state[\"user_problem\"]}\n",
                "**Task**: {domain_task.get('task_description', 'Design infrastructure')}\n",
                "**Requirements**: {', '.join(domain_task.get('requirements', []))}\n",
                "{feedback_context}\n",
                "Design detailed {domain} infrastructure. Use web_search or RAG_search if needed.\n",
                "\"\"\"\n",
                "        local_messages = [SystemMessage(content=system_prompt), HumanMessage(content=state[\"user_problem\"])]\n",
                "        tools_dict = tool_manager.get_all_tools()\n",
                "        llm_with_tools = llm_manager.get_mini_with_tools(list(tools_dict.values()))\n",
                "        \n",
                "        final_response = execute_tool_calls(local_messages, llm_with_tools, tools_dict, timeout=timeout)\n",
                "        content = getattr(final_response, \"content\", \"\")\n",
                "        recommendations = format_component_recommendations(domain, domain_task, content)\n",
                "        \n",
                "        return {\"architecture_components\": {domain: {\"recommendations\": recommendations, \"domain\": domain, \"task_info\": domain_task}}}\n",
                "    except Exception as e:\n",
                "        logger.error(f\"{domain} architect error: {e}\")\n",
                "        return {\"architecture_components\": {domain: {\"recommendations\": f\"Error: {e}\", \"error\": str(e)}}}\n",
                "\n",
                "def compute_architect(state: ArchitectureState, llm_manager, tool_manager) -> ArchitectureState:\n",
                "    return generic_domain_architect(state, \"compute\", \"EC2, Lambda, ECS, EKS\", llm_manager, tool_manager)\n",
                "\n",
                "def network_architect(state: ArchitectureState, llm_manager, tool_manager) -> ArchitectureState:\n",
                "    return generic_domain_architect(state, \"network\", \"VPC, ALB, Route 53\", llm_manager, tool_manager)\n",
                "\n",
                "def storage_architect(state: ArchitectureState, llm_manager, tool_manager) -> ArchitectureState:\n",
                "    return generic_domain_architect(state, \"storage\", \"S3, EBS, EFS\", llm_manager, tool_manager)\n",
                "\n",
                "def database_architect(state: ArchitectureState, llm_manager, tool_manager) -> ArchitectureState:\n",
                "    return generic_domain_architect(state, \"database\", \"RDS, DynamoDB, ElastiCache\", llm_manager, tool_manager)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Validators (`nodes/validators.py`)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def get_services_for_domain(domain: str) -> List[str]:\n",
                "    return {\n",
                "        \"compute\": [\"EC2\", \"Lambda\", \"ECS\"],\n",
                "        \"network\": [\"VPC\", \"Security Groups\", \"ALB\"],\n",
                "        \"storage\": [\"S3\", \"EBS\"],\n",
                "        \"database\": [\"RDS\", \"DynamoDB\"]\n",
                "    }.get(domain, [])\n",
                "\n",
                "def get_validation_focus(domain: str) -> str:\n",
                "    return {\n",
                "        \"compute\": \"Sizing, scaling, cost\",\n",
                "        \"network\": \"Security, routing\",\n",
                "        \"storage\": \"Access control, encryption\",\n",
                "        \"database\": \"HA, backups\"\n",
                "    }.get(domain, \"general validation\")\n",
                "\n",
                "def validator_supervisor(state: ArchitectureState, llm_manager) -> ArchitectureState:\n",
                "    logger.info(\"--- Validator Supervisor ---\")\n",
                "    try:\n",
                "        comps = state.get(\"architecture_components\", {})\n",
                "        val_tasks = {}\n",
                "        for domain in comps.keys():\n",
                "            val_tasks[domain] = {\n",
                "                \"components_to_validate\": get_services_for_domain(domain),\n",
                "                \"validation_focus\": get_validation_focus(domain)\n",
                "            }\n",
                "        \n",
                "        # Simple merge mimic since we are in one file\n",
                "        prev = state.get(\"architecture_domain_tasks\", {})\n",
                "        merged = prev.copy()\n",
                "        merged[\"validation_tasks\"] = val_tasks\n",
                "        return {\"architecture_domain_tasks\": merged}\n",
                "    except Exception as e:\n",
                "        return {\"architecture_domain_tasks\": state.get(\"architecture_domain_tasks\", {})}\n",
                "\n",
                "def generic_domain_validator(state: ArchitectureState, domain: str, llm_manager, tool_manager, timeout: float = 60.0) -> ArchitectureState:\n",
                "    logger.info(f\"--- {domain.capitalize()} Validator ---\")\n",
                "    try:\n",
                "        recs = state.get(\"architecture_components\", {}).get(domain, {}).get(\"recommendations\", \"\")\n",
                "        if not recs:\n",
                "             return {\"validation_feedback\": [{\"domain\": domain, \"status\": \"skipped\", \"has_errors\": False}]}\n",
                "\n",
                "        system_prompt = f\"\"\"\n",
                "Validate aws architecture.\n",
                "Domain: {domain}\n",
                "Focus: {get_validation_focus(domain)}\n",
                "Recommendation: {recs}\n",
                "Check for errors. Report issues.\n",
                "\"\"\"\n",
                "        tools = tool_manager.get_all_tools()\n",
                "        rag_only = {\"RAG_search\": tools.get(\"RAG_search\", tools[\"web_search\"])}\n",
                "        llm = llm_manager.get_mini_with_tools(list(rag_only.values()))\n",
                "        response = execute_tool_calls([SystemMessage(content=system_prompt)], llm, rag_only, timeout=timeout)\n",
                "        \n",
                "        val_result = getattr(response, \"content\", \"Validation done\")\n",
                "        has_errors = detect_errors_llm(val_result)\n",
                "        return {\"validation_feedback\": [{\"domain\": domain, \"result\": val_result, \"has_errors\": has_errors}], \"has_validation_errors\": has_errors}\n",
                "    except Exception as e:\n",
                "        return {\"validation_feedback\": [{\"domain\": domain, \"result\": f\"Error: {e}\", \"has_errors\": True}], \"has_validation_errors\": True}\n",
                "\n",
                "def compute_validator(state, llm, tools): return generic_domain_validator(state, \"compute\", llm, tools)\n",
                "def network_validator(state, llm, tools): return generic_domain_validator(state, \"network\", llm, tools)\n",
                "def storage_validator(state, llm, tools): return generic_domain_validator(state, \"storage\", llm, tools)\n",
                "def database_validator(state, llm, tools): return generic_domain_validator(state, \"database\", llm, tools)\n",
                "\n",
                "def validation_synthesizer(state: ArchitectureState, llm_manager) -> ArchitectureState:\n",
                "    logger.info(\"--- Validation Synthesizer ---\")\n",
                "    try:\n",
                "        all_fb = state.get(\"validation_feedback\", [])\n",
                "        if not all_fb: return {\"has_validation_errors\": False}\n",
                "        \n",
                "        error_count = sum(1 for fb in all_fb if fb.get(\"has_errors\"))\n",
                "        fb_text = \"\\n\".join([f\"{fb['domain']}: {fb['result'][:200]}\" for fb in all_fb])\n",
                "        \n",
                "        system_prompt = f\"Summarize validation results:\\n{fb_text}\\nErrors: {error_count}\"\n",
                "        response = llm_manager.get_reasoning_llm().invoke([SystemMessage(content=system_prompt)])\n",
                "        return {\"validation_summary\": getattr(response, \"content\", \"\"), \"has_validation_errors\": error_count > 0}\n",
                "    except Exception as e:\n",
                "        return {\"validation_summary\": str(e), \"has_validation_errors\": True}"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Synthesizer & Supervisor (`nodes/synthesizer.py`, `nodes/supervisor.py`)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def architect_synthesizer(state: ArchitectureState, llm_manager) -> ArchitectureState:\n",
                "    logger.info(\"--- Architect Synthesizer ---\")\n",
                "    try:\n",
                "        comps = state.get(\"architecture_components\", {})\n",
                "        if not comps:\n",
                "             return {\"proposed_architecture\": {\"architecture_summary\": \"No components\"}}\n",
                "        \n",
                "        summaries = []\n",
                "        for d, i in comps.items():\n",
                "            summaries.append(f\"**{d.upper()}**: {i.get('recommendations', 'N/A')}\")\n",
                "            \n",
                "        system_prompt = f\"\"\"\n",
                "Synthesize these components into one architecture:\n",
                "{''.join(summaries)}\n",
                "Problem: {state['user_problem']}\n",
                "\"\"\"\n",
                "        response = llm_manager.get_reasoning_llm().invoke([SystemMessage(content=system_prompt)])\n",
                "        return {\"proposed_architecture\": {\"architecture_summary\": response.content, \"source_components\": comps}}\n",
                "    except Exception as e:\n",
                "        return {\"proposed_architecture\": {\"architecture_summary\": f\"Error: {e}\"}}\n",
                "\n",
                "def architect_supervisor(state: ArchitectureState, llm_manager, max_retries: int = 3) -> ArchitectureState:\n",
                "    logger.info(\"--- Architect Supervisor ---\")\n",
                "    try:\n",
                "        system_prompt = f\"\"\"\n",
                "Break down problem into tasks for Compute, Network, Storage, Database.\n",
                "Problem: {state['user_problem']}\n",
                "\"\"\"\n",
                "        structured = llm_manager.get_reasoning_structured(TaskDecomposition)\n",
                "        response = structured.invoke([SystemMessage(content=system_prompt)])\n",
                "        \n",
                "        updates = {\"overall_goals\": response.overall_architecture_goals, \"constraints\": response.constraints}\n",
                "        for t in response.decomposed_tasks:\n",
                "            updates[t.domain.lower()] = {\"task_description\": t.task_description, \"requirements\": t.requirements, \"deliverables\": t.deliverables}\n",
                "            \n",
                "        return {\n",
                "            \"architecture_domain_tasks\": updates,\n",
                "            \"iteration_count\": state[\"iteration_count\"] + 1,\n",
                "            \"validation_feedback\": [],\n",
                "            \"architecture_components\": {},\n",
                "            \"has_validation_errors\": False\n",
                "        }\n",
                "    except Exception as e:\n",
                "        logger.error(f\"Supervisor failed: {e}\")\n",
                "        return {\"iteration_count\": state[\"iteration_count\"] + 1, \"has_validation_errors\": True}"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 8. Iteration & Graph (`nodes/iteration.py`, `graph/builder.py`)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def iteration_condition(state: ArchitectureState) -> Literal[\"iterate\", \"finish\"]:\n",
                "    itr = state.get(\"iteration_count\", 0)\n",
                "    min_i = state.get(\"min_iterations\", 1)\n",
                "    max_i = state.get(\"max_iterations\", 3)\n",
                "    err = state.get(\"has_validation_errors\", False)\n",
                "    \n",
                "    if itr < min_i: return \"iterate\"\n",
                "    if err and itr < max_i: return \"iterate\"\n",
                "    return \"finish\"\n",
                "\n",
                "def final_architecture_generator(state: ArchitectureState, llm_manager) -> ArchitectureState:\n",
                "    logger.info(\"--- Final Generator ---\")\n",
                "    try:\n",
                "        summary = state.get(\"proposed_architecture\", {}).get(\"architecture_summary\", \"\")\n",
                "        response = llm_manager.get_reasoning_llm().invoke([SystemMessage(content=f\"Create final doc based on: {summary}\")])\n",
                "        return {\"final_architecture\": {\"document\": response.content}}\n",
                "    except Exception as e:\n",
                "        return {\"final_architecture\": {\"document\": str(e)}}\n",
                "\n",
                "def build_architecture_graph(llm_manager, tool_manager):\n",
                "    logger.info(\"Building Graph...\")\n",
                "    builder = StateGraph(ArchitectureState)\n",
                "    \n",
                "    builder.add_node(\"architect_supervisor\", lambda s: architect_supervisor(s, llm_manager))\n",
                "    builder.add_node(\"compute_architect\", lambda s: compute_architect(s, llm_manager, tool_manager))\n",
                "    builder.add_node(\"network_architect\", lambda s: network_architect(s, llm_manager, tool_manager))\n",
                "    builder.add_node(\"storage_architect\", lambda s: storage_architect(s, llm_manager, tool_manager))\n",
                "    builder.add_node(\"database_architect\", lambda s: database_architect(s, llm_manager, tool_manager))\n",
                "    builder.add_node(\"architect_synthesizer\", lambda s: architect_synthesizer(s, llm_manager))\n",
                "    builder.add_node(\"validator_supervisor\", lambda s: validator_supervisor(s, llm_manager))\n",
                "    builder.add_node(\"compute_validator\", lambda s: compute_validator(s, llm_manager, tool_manager))\n",
                "    builder.add_node(\"network_validator\", lambda s: network_validator(s, llm_manager, tool_manager))\n",
                "    builder.add_node(\"storage_validator\", lambda s: storage_validator(s, llm_manager, tool_manager))\n",
                "    builder.add_node(\"database_validator\", lambda s: database_validator(s, llm_manager, tool_manager))\n",
                "    builder.add_node(\"validation_synthesizer\", lambda s: validation_synthesizer(s, llm_manager))\n",
                "    builder.add_node(\"final_architecture_generator\", lambda s: final_architecture_generator(s, llm_manager))\n",
                "    \n",
                "    builder.add_edge(START, \"architect_supervisor\")\n",
                "    builder.add_edge(\"architect_supervisor\", \"compute_architect\")\n",
                "    builder.add_edge(\"architect_supervisor\", \"network_architect\")\n",
                "    builder.add_edge(\"architect_supervisor\", \"storage_architect\")\n",
                "    builder.add_edge(\"architect_supervisor\", \"database_architect\")\n",
                "    builder.add_edge(\"compute_architect\", \"architect_synthesizer\")\n",
                "    builder.add_edge(\"network_architect\", \"architect_synthesizer\")\n",
                "    builder.add_edge(\"storage_architect\", \"architect_synthesizer\")\n",
                "    builder.add_edge(\"database_architect\", \"architect_synthesizer\")\n",
                "    builder.add_edge(\"architect_synthesizer\", \"validator_supervisor\")\n",
                "    builder.add_edge(\"validator_supervisor\", \"compute_validator\")\n",
                "    builder.add_edge(\"validator_supervisor\", \"network_validator\")\n",
                "    builder.add_edge(\"validator_supervisor\", \"storage_validator\")\n",
                "    builder.add_edge(\"validator_supervisor\", \"database_validator\")\n",
                "    builder.add_edge(\"compute_validator\", \"validation_synthesizer\")\n",
                "    builder.add_edge(\"network_validator\", \"validation_synthesizer\")\n",
                "    builder.add_edge(\"storage_validator\", \"validation_synthesizer\")\n",
                "    builder.add_edge(\"database_validator\", \"validation_synthesizer\")\n",
                "    \n",
                "    builder.add_conditional_edges(\n",
                "        \"validation_synthesizer\",\n",
                "        iteration_condition,\n",
                "        {\"iterate\": \"architect_supervisor\", \"finish\": \"final_architecture_generator\"}\n",
                "    )\n",
                "    builder.add_edge(\"final_architecture_generator\", END)\n",
                "    \n",
                "    return builder.compile(checkpointer=MemorySaver())"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 9. Main System (`main.py`)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class ArchitectureGenerationSystem:\n",
                "    def __init__(self):\n",
                "        logger.info(\"Initializing System...\")\n",
                "        self.tool_manager = ToolManager()\n",
                "        self.llm_manager = LLMManager()\n",
                "        self.graph = build_architecture_graph(self.llm_manager, self.tool_manager)\n",
                "    \n",
                "    def run(self, user_problem, thread_id=None, min_iterations=1, max_iterations=2):\n",
                "        if thread_id is None: thread_id = f\"arch-{int(time.time())}\"\n",
                "        config = {\"configurable\": {\"thread_id\": thread_id}}\n",
                "        \n",
                "        state = create_initial_state(user_problem, min_iterations, max_iterations)\n",
                "        logger.info(f\"Starting run {thread_id}...\")\n",
                "        return self.graph.invoke(state, config=config)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 10. Run It"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from dotenv import load_dotenv\n",
                "load_dotenv()\n",
                "\n",
                "# Ensure you have OPENAI_API_KEY set in your .env or environment\n",
                "system = ArchitectureGenerationSystem()\n",
                "\n",
                "result = system.run(\n",
                "    user_problem=\"Build a highly available e-commerce platform on AWS\",\n",
                "    thread_id=\"notebook-run-1\",\n",
                "    min_iterations=1,\n",
                "    max_iterations=1  # Keeping it short for testing\n",
                ")\n",
                "\n",
                "print(\"\\n--- FINAL DOCUMENT ---\\n\")\n",
                "print(result.get(\"final_architecture\", {}).get(\"document\", \"No document generated\"))"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.12"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}